# ğŸš€ AI-Powered RAG Pipeline

## ğŸ“¸ Demo

![demo](Image/demo.gif)

## ğŸ“¸ UI Exposure

![Demo Screenshot](Image/app_rag.png)

## ğŸ“¸ RAG Response on User Query

![Demo Screenshot](Image/app_rag_response.png)

## âš™ï¸ Features

- ğŸ” PDF/Text Ingestion using LlamaIndex
- ğŸ§  Smart Retrieval using FAISS
- ğŸ’¬ Query with OpenAI GPT-3.5
- ğŸ“¦ Modular Code Structure
- ğŸš€ Easy Streamlit UI (Optional)



## ğŸ“¦ Tech Stack

- LlamaIndex
- FAISS
- OpenAI GPT
- Streamlit
- Python 3.10+

## ğŸ› ï¸ Installation

```bash
git clone https://github.com/subhasish-creator/RAG_pipeline.git
cd RAG_pipeline
pip install -r requirements.txt
streamlit run app_rag.py --server.address 127.0.0.1 --server.port 8502

## âš™ï¸ Folder Structure Information at High level

- ğŸ” RAG folder consist components, config,data,modules and utils blocks
- ğŸ” data folder hold the supplied data file (PDF or Txt)
- ğŸ” config folder hold config.yaml file. This file talk about LLM model name and its parameter.
- ğŸ” modules\config_loader.py load the LLM model as perconfiguration of yaml file
- ğŸ” modules\data_loader.py load the data file needed for embedding
- ğŸ” modules\vectore_store.py is use for VectorDB creation
- ğŸ” modules\query_engine.py is use for handling query
- ğŸ” modules\task2.py is use for storing Database locally
- ğŸ” Storage folder store knowledgeDB created by VectorStorIndex locally in HardDisk
- ğŸ” app.py for UI part of LLM
  
